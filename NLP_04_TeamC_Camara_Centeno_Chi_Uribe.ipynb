{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"upy2.png\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Universidad Politécnica de Yucatán </center>\n",
    "### <center> Data Engineering </center>\n",
    "### <center> Natural Language Processing </center>\n",
    "### <center> Isabel Cámara Salinas </center>\n",
    "### <center> Ricardo Armando Centeno Santos </center>\n",
    "### <center> Mayte Alejandra Chi Poot </center>\n",
    "### <center> Victor Rodrigo Uribe Hernández </center>\n",
    "### <center> 9th quarter </center>\n",
    "### <center> Mario Campos Soberanis </center>\n",
    "### <center> November 11, 2022 </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        The PreProcessor class provides different methods to preprocess a string that the method will receive.\n",
    "        Attributes:\n",
    "            pos: stores the position that we assign for the lemmatization. Our base case is the position 'noun', because\n",
    "            at the moment that we call the lemmatize method it is more common and useful to lemmatize in nouns. This\n",
    "            position could be modified by the user.\n",
    "            special_chars: stores the characters that we want to remove to the strings in the remove noise method.\n",
    "            regex_dict: stores the dictionary of the regular expressions that we want to extract.\n",
    "        \n",
    "        Methods:\n",
    "            stemWords: It receives a string and transforms the words to its root form.\n",
    "            lemmatizeWords: It receives a string and reduces the words to a word existing in the language.\n",
    "            removeNoise: It receives a string and remove all the characters that are included in the attribute of special\n",
    "            characters to obtain a cleaned string.\n",
    "            wordTokenize: It receives a string and splits into words.\n",
    "            phraseTokenize: It receives a string and splits into phrases.\n",
    "            textNormalization: It receives a string and removes the regular expressions that we previously defined in \n",
    "            the attribute of regex dict to obtain a string with more coherence.\n",
    "            extractRegex: It receives a string and extract the regula expressions that we defined in the previous attributes.\n",
    "            cleaning: This method calls the methods that we consider are necessary for the preprocessing of the tweet \n",
    "            database.\n",
    "        '''\n",
    "        self.pos = 'n'\n",
    "        self.special_chars = \",.@?!¬-\\''=()\"\n",
    "        self.regex_dict = {'Tags' : r'@[A-Za-z0-9]+', \n",
    "                      '# symbol' : r'#', \n",
    "                      'RT' : r'RT', \n",
    "                      'Links' : r'https?://\\S+',\n",
    "                      'Not letters': r'[^A-Za-z\\s]+',\n",
    "                      'Phone' : r'\\+[0-9]{12}'}\n",
    "    \n",
    "    def stemWords(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives a string and call the PorterStemmer function of the nltk library of python to do the stemming \n",
    "        process. We call the word tokenize method to split the string and obtain the root of each word, and then when\n",
    "        the words are stemmed, we join the string again.\n",
    "        Return: Stemmed string\n",
    "        '''\n",
    "        ps = PorterStemmer()\n",
    "        stem = list(map(ps.stem, self.wordTokenize(string)))\n",
    "        stemmed = ' '.join(stem)\n",
    "        return stemmed\n",
    "    \n",
    "    def lemmatizeWords(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives a string and call the WordNetLemmatizer function of the nltk library of python to do the \n",
    "        lemmatizing process which is going to receive the word and the attribute of position. We call the word tokenize\n",
    "        method to split the string and obtain the word of the language of each word, and then when the words are \n",
    "        lemmatized, we join the string again.\n",
    "        Return: Lemmatized string\n",
    "        '''\n",
    "        wnl = WordNetLemmatizer()\n",
    "        lemm = [wnl.lemmatize(word, self.pos) for word in self.wordTokenize(string)]\n",
    "        lematized = ' '.join(lemm)\n",
    "        return lematized\n",
    "    \n",
    "    def removeNoise(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives the string and makes all the characters lower, then check the string and if there are \n",
    "        characters that are part of the attribute of special characters, it replaces the character with nothing and\n",
    "        finally join the string again.\n",
    "        Return: cleaned string\n",
    "        '''\n",
    "        clean_string = string.lower()\n",
    "        for char in self.special_chars:\n",
    "            clean_string = clean_string.replace(char, \"\")\n",
    "        splitted = self.wordTokenize(clean_string)\n",
    "        cleaned = [w.replace(\" \", \"\") for w in splitted if len(w) > 0]\n",
    "        clean_string = \" \".join(cleaned)\n",
    "        return clean_string\n",
    "    \n",
    "    def wordTokenize(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives the string and split all the words adding the words to the list, using the word tokenize \n",
    "        function of the nltk.\n",
    "        Return: list of tokenized words\n",
    "        '''\n",
    "        tokenized = word_tokenize(string)\n",
    "        return tokenized\n",
    "    \n",
    "    def phraseTokenize(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives the string and split the phrases using the reference '. ' and then adding the phrase to the list\n",
    "        Return: list of tokenized phrases\n",
    "        '''\n",
    "        cleaned = string.split('. ')\n",
    "        return cleaned\n",
    "    \n",
    "    def textNormalization(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives and splits the string and remove the regular expressions of the string. To make this, \n",
    "        it iterates over the attribute that stores the regular expression dictionary.\n",
    "        Return: String without the regular expresions\n",
    "        '''\n",
    "        for key in self.regex_dict.keys():\n",
    "            string = re.sub(self.regex_dict[key], '', string)\n",
    "        normalized =  \" \".join(self.wordTokenize(string))\n",
    "        return normalized\n",
    "    \n",
    "    def extractRegex(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives the string, creates a dictionary to store the regular expressions that we extract of each\n",
    "        string and calls the method of text nrmalization to also generate a cleaned string that we also will return to\n",
    "        the user.\n",
    "        Return: dictionary with the regular expressions that we extract and a cleaned string without the regular expressions\n",
    "        '''\n",
    "        dict_found_strings = dict()\n",
    "        \n",
    "        for key in self.regex_dict.keys():\n",
    "            found_strings = re.findall(self.regex_dict[key], string)\n",
    "            dict_found_strings[key] = found_strings\n",
    "\n",
    "        replaced_string = self.textNormalization(string = string)\n",
    "        return dict_found_strings, replaced_string\n",
    "    \n",
    "    def cleaning(self, data):\n",
    "        '''\n",
    "        Input: Data\n",
    "        Process: This method call the methods that we consider necessary for the preprocessing of the tweets database.\n",
    "        In this case we select the text normalization, the stemmatization words, and the remove noise methods. This \n",
    "        method receives in this case a column of the dataframe and returns the column of the dataframe preprocesed.\n",
    "        Return: Preprocessed Data\n",
    "        '''\n",
    "        #text Normalization\n",
    "        data = data.apply(self.textNormalization)\n",
    "        \n",
    "        #stem words\n",
    "        data = data.apply(self.stemWords)\n",
    "        \n",
    "        #Remove Noise\n",
    "        data = data.apply(self.removeNoise)\n",
    "        \n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweets.csv\")\n",
    "tweets = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = PreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>senti</th>\n",
       "      <th>cleaning_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@united Oh, we are sure it's not planned, but ...</td>\n",
       "      <td>0</td>\n",
       "      <td>oh we are sure it not plan but it occur absolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>History exam studying ugh</td>\n",
       "      <td>0</td>\n",
       "      <td>histori exam studi ugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@unnitallman yeah looks like that only! &amp;quot;...</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah look like that onli quotbusyquot is fuck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Loves twitter</td>\n",
       "      <td>4</td>\n",
       "      <td>love twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Mbjthegreat i really dont want AT&amp;amp;T phone...</td>\n",
       "      <td>0</td>\n",
       "      <td>i realli dont want atampt phone servicethey su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>@criticalpath Such an awesome idea - the  cont...</td>\n",
       "      <td>4</td>\n",
       "      <td>such an awesom idea the continu learn program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Talk is Cheap: Bing that, I?ll stick with Goog...</td>\n",
       "      <td>0</td>\n",
       "      <td>talk is cheap bing that ill stick with googl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>@CTesdahl well you can always digg or stumble....</td>\n",
       "      <td>4</td>\n",
       "      <td>well you can alway digg or stumbleboth have ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Stopped to have lunch at McDonalds. Chicken Nu...</td>\n",
       "      <td>4</td>\n",
       "      <td>stop to have lunch at mcdonald chicken nuggets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>SUPER INVESTORS: A great weekend read here fro...</td>\n",
       "      <td>4</td>\n",
       "      <td>super investor a great weekend read here from ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  senti  \\\n",
       "0    @united Oh, we are sure it's not planned, but ...      0   \n",
       "1                            History exam studying ugh      0   \n",
       "2    @unnitallman yeah looks like that only! &quot;...      0   \n",
       "3                                        Loves twitter      4   \n",
       "4    @Mbjthegreat i really dont want AT&amp;T phone...      0   \n",
       "..                                                 ...    ...   \n",
       "293  @criticalpath Such an awesome idea - the  cont...      4   \n",
       "294  Talk is Cheap: Bing that, I?ll stick with Goog...      0   \n",
       "295  @CTesdahl well you can always digg or stumble....      4   \n",
       "296  Stopped to have lunch at McDonalds. Chicken Nu...      4   \n",
       "297  SUPER INVESTORS: A great weekend read here fro...      4   \n",
       "\n",
       "                                       cleaning_tweets  \n",
       "0    oh we are sure it not plan but it occur absolu...  \n",
       "1                               histori exam studi ugh  \n",
       "2    yeah look like that onli quotbusyquot is fuck ...  \n",
       "3                                         love twitter  \n",
       "4    i realli dont want atampt phone servicethey su...  \n",
       "..                                                 ...  \n",
       "293  such an awesom idea the continu learn program ...  \n",
       "294       talk is cheap bing that ill stick with googl  \n",
       "295  well you can alway digg or stumbleboth have ai...  \n",
       "296  stop to have lunch at mcdonald chicken nuggets...  \n",
       "297  super investor a great weekend read here from ...  \n",
       "\n",
       "[298 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaning_tweets'] = prep.cleaning(tweets)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti</th>\n",
       "      <th>cleaning_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh we are sure it not plan but it occur absolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>histori exam studi ugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>yeah look like that onli quotbusyquot is fuck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>love twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i realli dont want atampt phone servicethey su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>4</td>\n",
       "      <td>such an awesom idea the continu learn program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0</td>\n",
       "      <td>talk is cheap bing that ill stick with googl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4</td>\n",
       "      <td>well you can alway digg or stumbleboth have ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>4</td>\n",
       "      <td>stop to have lunch at mcdonald chicken nuggets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>4</td>\n",
       "      <td>super investor a great weekend read here from ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     senti                                    cleaning_tweets\n",
       "0        0  oh we are sure it not plan but it occur absolu...\n",
       "1        0                             histori exam studi ugh\n",
       "2        0  yeah look like that onli quotbusyquot is fuck ...\n",
       "3        4                                       love twitter\n",
       "4        0  i realli dont want atampt phone servicethey su...\n",
       "..     ...                                                ...\n",
       "293      4  such an awesom idea the continu learn program ...\n",
       "294      0       talk is cheap bing that ill stick with googl\n",
       "295      4  well you can alway digg or stumbleboth have ai...\n",
       "296      4  stop to have lunch at mcdonald chicken nuggets...\n",
       "297      4  super investor a great weekend read here from ...\n",
       "\n",
       "[298 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['tweet'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing the data set into training and test sets\n",
    "\n",
    "We divided the dataset in two parts. The first one is the training set; this one has the 90% of the data. The second one is the test set; this one has the 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti</th>\n",
       "      <th>cleaning_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2</td>\n",
       "      <td>adob cs commerci by goodbi silverstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>4</td>\n",
       "      <td>i would like to go for a drive with these guy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>2</td>\n",
       "      <td>googl vision deliv packag with driverless car ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "      <td>wtf is the point of delet tweet if they can st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "      <td>i just wrote and entir stori and it didnt save...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>4</td>\n",
       "      <td>such an awesom idea the continu learn program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0</td>\n",
       "      <td>talk is cheap bing that ill stick with googl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4</td>\n",
       "      <td>well you can alway digg or stumbleboth have ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>4</td>\n",
       "      <td>stop to have lunch at mcdonald chicken nuggets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>4</td>\n",
       "      <td>super investor a great weekend read here from ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     senti                                    cleaning_tweets\n",
       "218      2             adob cs commerci by goodbi silverstein\n",
       "219      4  i would like to go for a drive with these guy ...\n",
       "220      2  googl vision deliv packag with driverless car ...\n",
       "221      0  wtf is the point of delet tweet if they can st...\n",
       "222      0  i just wrote and entir stori and it didnt save...\n",
       "..     ...                                                ...\n",
       "293      4  such an awesom idea the continu learn program ...\n",
       "294      0       talk is cheap bing that ill stick with googl\n",
       "295      4  well you can alway digg or stumbleboth have ai...\n",
       "296      4  stop to have lunch at mcdonald chicken nuggets...\n",
       "297      4  super investor a great weekend read here from ...\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df[218:298]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti</th>\n",
       "      <th>cleaning_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>oh we are sure it not plan but it occur absolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>histori exam studi ugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>yeah look like that onli quotbusyquot is fuck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>love twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i realli dont want atampt phone servicethey su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0</td>\n",
       "      <td>go to the dentist later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2</td>\n",
       "      <td>that look an aw lot like one of nike privat je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2</td>\n",
       "      <td>i thought it wa a selfdriv carlt must be a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2</td>\n",
       "      <td>watch a programm about the life of hitler it o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>4</td>\n",
       "      <td>i love read your tweet but it would be nice to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     senti                                    cleaning_tweets\n",
       "0        0  oh we are sure it not plan but it occur absolu...\n",
       "1        0                             histori exam studi ugh\n",
       "2        0  yeah look like that onli quotbusyquot is fuck ...\n",
       "3        4                                       love twitter\n",
       "4        0  i realli dont want atampt phone servicethey su...\n",
       "..     ...                                                ...\n",
       "263      0                            go to the dentist later\n",
       "264      2  that look an aw lot like one of nike privat je...\n",
       "265      2  i thought it wa a selfdriv carlt must be a man...\n",
       "266      2  watch a programm about the life of hitler it o...\n",
       "267      4  i love read your tweet but it would be nice to...\n",
       "\n",
       "[268 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(range(268,298),axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = df.to_numpy().tolist()\n",
    "c = df['senti'].to_numpy().tolist()\n",
    "C = list(set(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes training model \n",
    "\n",
    "This is the model that we are going to use to train the data. We are going to use the Naive Bayes algorithm.\n",
    "First, we initialized the \"logprior\" and the \"loglikelihood\".The logprior is the sum of the loglikelihood of each word in the training set. The loglikelihood is the sum of the loglikelihood of each word in the training set. Then, C are the classes that we have in the dataset, in this case, are the classes of the sentiment analysis (0,2,4).\n",
    "\n",
    "This training model is in charge of detecting the different classes of the tweets found in the dataset, in this case, the classes refer to the sentiment analysis levels assigned to each tweet. Once this is done, it generates a vocabulary to be stored and tested later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(D, C):\n",
    "    # initialize logprior, loglikelihood\n",
    "    logprior = {}\n",
    "    loglikelihood = {}\n",
    "    V = set()\n",
    "    \n",
    "    # for each class c in C\n",
    "    for c in C:\n",
    "        if c not in loglikelihood:\n",
    "            loglikelihood[c] = {}\n",
    "        N_doc = len(D)\n",
    "        #number of documents from D in class C\n",
    "        N_c = 0\n",
    "\n",
    "        # for each document d in D\n",
    "        for d in D:\n",
    "            # if document d is in class c\n",
    "            if d[0] == c:\n",
    "                # increment N_c\n",
    "                N_c += 1\n",
    "                # for each word w in d\n",
    "                #for w in d[1]:\n",
    "                    #print(w)\n",
    "                    #w.split(' ')\n",
    "                    # add word w to V\n",
    "                w = d[1]\n",
    "                w = w.split(' ')\n",
    "                for word in w:\n",
    "                    V.add(word)\n",
    "                    # if word w is not in loglikelihood[c]\n",
    "                    if word not in loglikelihood[c]:\n",
    "                        # add word w to loglikelihood[c]\n",
    "                        loglikelihood[c][word] = 0\n",
    "                    # increment loglikelihood[c][w]\n",
    "                    loglikelihood[c][word] += 1\n",
    "        # compute logprior[c]\n",
    "        logprior[c] = np.log(N_c/N_doc)\n",
    "\n",
    "        # for each word w in V\n",
    "        for word in V:\n",
    "            # compute loglikelihood[c][w]\n",
    "            if word not in loglikelihood[c]:\n",
    "                loglikelihood[c][word] = np.log((1)/(len(V) + 1))\n",
    "            else:\n",
    "                loglikelihood[c][word] = np.log((loglikelihood[c][word] + 1)/(len(V) + 1))\n",
    "    return logprior, loglikelihood, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikelihood, V = train_naive_bayes(D, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -0.9660141672265856, 2: -1.4165997106152195, 4: -0.9758664636695973}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes test model\n",
    "Once the training is done, we apply the Naive Bayes testing model. This will be in charge of classifying the classes previously added in the training and counting the number of classes in the training. Our classes are 0, 2 and 4. The 0 class is the negative sentiment, the 2 class is the neutral sentiment and the 4 class is the positive sentiment. Finally, we create the variable \"test_doc\" to store the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_naive_bayes(testdoc, logprior, loglikelihood, C,V):\n",
    "    #output array containing the probability for each class\n",
    "    suma = np.zeros(len(C))# [0,0,0]\n",
    "    # for each class c in C\n",
    "    for i, c in enumerate(C):#[0,2,4]\n",
    "        # compute logprior[c]\n",
    "        suma[i] = logprior[c]\n",
    "        # for each word w in testdoc\n",
    "        w = testdoc.split(' ')\n",
    "        for word in w:\n",
    "            # if w is in V\n",
    "            if word in V:\n",
    "                # compute loglikelihood[c][w]\n",
    "                if word in loglikelihood[c]:\n",
    "                    \n",
    "                    suma[i] += loglikelihood[c][word]\n",
    "                else:\n",
    "                    suma[i] = 1\n",
    "                \n",
    "    return suma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = test['cleaning_tweets'].to_numpy().tolist()\n",
    "test_values = test['senti'].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes prediction model\n",
    "\n",
    "The \"labeling\" function takes the previously created array and chooses which one has the highest value and generates a label depending on its position. The three positions that our model is 0, 2 and 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(array):\n",
    "    array = [-500 if element == 1 else element for element in array]\n",
    "    array = list(array)\n",
    "    maximum = max(array)\n",
    "    pos = array.index(maximum)\n",
    "    return [0, 2, 4][pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_predicted = []\n",
    "for element in test_doc:\n",
    "    naive = testing_naive_bayes(element, logprior, loglikelihood, C, V)\n",
    "    labels_predicted.append(labeling(naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy, precision, and recall\n",
    "After the training and testing of the model, we calculate the accuracy of the model. The accuracy is the number of correct predictions divided by the total number of predictions. The accuracy of our model is 0.66. Our recall is 0.65 and our precision is 0.72. Our results are not very good, but we think that it is because we are using a small dataset. We think that if we use a bigger dataset, our results will be better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is:  0.6625\n"
     ]
    }
   ],
   "source": [
    "accuracy_score(test_values, labels_predicted)\n",
    "print(\"The accuracy of the model is: \", accuracy_score(test_values, labels_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall of the model is:  0.6541847041847042\n"
     ]
    }
   ],
   "source": [
    "recall_score(test_values, labels_predicted, average = 'macro')\n",
    "print(\"The recall of the model is: \", recall_score(test_values, labels_predicted, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision of the model is:  0.726890756302521\n"
     ]
    }
   ],
   "source": [
    "precision_score(test_values, labels_predicted, average = 'macro')\n",
    "print(\"The precision of the model is: \", precision_score(test_values, labels_predicted, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "Finally, we have the confusion matrix. This matrix is in charge of showing the number of correct and incorrect predictions. The diagonal of the matrix shows the number of correct predictions. The rest of the matrix shows the number of incorrect predictions. The confusion matrix shows that the model has a good performance in the 0 class, but it has a bad performance in the 2 and 4 classes. We think that this is because we are using a small dataset. We think that if we use a bigger dataset, our results will be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28,  0,  2],\n",
       "       [ 7, 14,  1],\n",
       "       [14,  3, 11]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_values, labels_predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4983ef1541e43841f9e934630005c5670a77666a511d6fa2b0487a6871a7991"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
