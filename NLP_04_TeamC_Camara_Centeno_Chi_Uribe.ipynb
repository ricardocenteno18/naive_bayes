{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"upy2.png\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Universidad Politécnica de Yucatán </center>\n",
    "### <center> Data Engineering </center>\n",
    "### <center> Natural Language Processing </center>\n",
    "### <center> Isabel Cámara Salinas </center>\n",
    "### <center> Ricardo Armando Centeno Santos </center>\n",
    "### <center> Mayte Alejandra Chi Poot </center>\n",
    "### <center> Victor Rodrigo Uribe Hernández </center>\n",
    "### <center> 9th quarter </center>\n",
    "### <center> Mario Campos Soberanis </center>\n",
    "### <center> November 10, 2022 </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        The PreProcessor class provides different methods to preprocess a string that the method will receive.\n",
    "        Attributes:\n",
    "            pos: stores the position that we assign for the lemmatization. Our base case is the position 'noun', because\n",
    "            at the moment that we call the lemmatize method it is more common and useful to lemmatize in nouns. This\n",
    "            position could be modified by the user.\n",
    "            special_chars: stores the characters that we want to remove to the strings in the remove noise method.\n",
    "            regex_dict: stores the dictionary of the regular expressions that we want to extract.\n",
    "        \n",
    "        Methods:\n",
    "            stemWords: It receives a string and transforms the words to its root form.\n",
    "            lemmatizeWords: It receives a string and reduces the words to a word existing in the language.\n",
    "            removeNoise: It receives a string and remove all the characters that are included in the attribute of special\n",
    "            characters to obtain a cleaned string.\n",
    "            wordTokenize: It receives a string and splits into words.\n",
    "            phraseTokenize: It receives a string and splits into phrases.\n",
    "            textNormalization: It receives a string and removes the regular expressions that we previously defined in \n",
    "            the attribute of regex dict to obtain a string with more coherence.\n",
    "            extractRegex: It receives a string and extract the regula expressions that we defined in the previous attributes.\n",
    "            cleaning: This method calls the methods that we consider are necessary for the preprocessing of the tweet \n",
    "            database.\n",
    "        '''\n",
    "        self.pos = 'n'\n",
    "        self.special_chars = \",.@?!¬-\\''=()\"\n",
    "        self.regex_dict = {'Tags' : r'@[A-Za-z0-9]+', \n",
    "                      '# symbol' : r'#', \n",
    "                      'RT' : r'RT', \n",
    "                      'Links' : r'https?://\\S+',\n",
    "                      'Not letters': r'[^A-Za-z\\s]+',\n",
    "                      'Phone' : r'\\+[0-9]{12}'}\n",
    "    \n",
    "    def stemWords(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives a string and call the PorterStemmer function of the nltk library of python to do the stemming \n",
    "        process. We call the word tokenize method to split the string and obtain the root of each word, and then when\n",
    "        the words are stemmed, we join the string again.\n",
    "        Return: Stemmed string\n",
    "        '''\n",
    "        ps = PorterStemmer()\n",
    "        stem = list(map(ps.stem, self.wordTokenize(string)))\n",
    "        stemmed = ' '.join(stem)\n",
    "        return stemmed\n",
    "    \n",
    "    def lemmatizeWords(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives a string and call the WordNetLemmatizer function of the nltk library of python to do the \n",
    "        lemmatizing process which is going to receive the word and the attribute of position. We call the word tokenize\n",
    "        method to split the string and obtain the word of the language of each word, and then when the words are \n",
    "        lemmatized, we join the string again.\n",
    "        Return: Lemmatized string\n",
    "        '''\n",
    "        wnl = WordNetLemmatizer()\n",
    "        lemm = [wnl.lemmatize(word, self.pos) for word in self.wordTokenize(string)]\n",
    "        lematized = ' '.join(lemm)\n",
    "        return lematized\n",
    "    \n",
    "    def removeNoise(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives the string and makes all the characters lower, then check the string and if there are \n",
    "        characters that are part of the attribute of special characters, it replaces the character with nothing and\n",
    "        finally join the string again.\n",
    "        Return: cleaned string\n",
    "        '''\n",
    "        clean_string = string.lower()\n",
    "        for char in self.special_chars:\n",
    "            clean_string = clean_string.replace(char, \"\")\n",
    "        splitted = self.wordTokenize(clean_string)\n",
    "        cleaned = [w.replace(\" \", \"\") for w in splitted if len(w) > 0]\n",
    "        clean_string = \" \".join(cleaned)\n",
    "        return clean_string\n",
    "    \n",
    "    def wordTokenize(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives the string and split all the words adding the words to the list, using the word tokenize \n",
    "        function of the nltk.\n",
    "        Return: list of tokenized words\n",
    "        '''\n",
    "        tokenized = word_tokenize(string)\n",
    "        return tokenized\n",
    "    \n",
    "    def phraseTokenize(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives the string and split the phrases using the reference '. ' and then adding the phrase to the list\n",
    "        Return: list of tokenized phrases\n",
    "        '''\n",
    "        cleaned = string.split('. ')\n",
    "        return cleaned\n",
    "    \n",
    "    def textNormalization(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives and splits the string and remove the regular expressions of the string. To make this, \n",
    "        it iterates over the attribute that stores the regular expression dictionary.\n",
    "        Return: String without the regular expresions\n",
    "        '''\n",
    "        for key in self.regex_dict.keys():\n",
    "            string = re.sub(self.regex_dict[key], '', string)\n",
    "        normalized =  \" \".join(self.wordTokenize(string))\n",
    "        return normalized\n",
    "    \n",
    "    def extractRegex(self, string):\n",
    "        '''\n",
    "        Input: String\n",
    "        Process: Receives the string, creates a dictionary to store the regular expressions that we extract of each\n",
    "        string and calls the method of text nrmalization to also generate a cleaned string that we also will return to\n",
    "        the user.\n",
    "        Return: dictionary with the regular expressions that we extract and a cleaned string without the regular expressions\n",
    "        '''\n",
    "        dict_found_strings = dict()\n",
    "        \n",
    "        for key in self.regex_dict.keys():\n",
    "            found_strings = re.findall(self.regex_dict[key], string)\n",
    "            dict_found_strings[key] = found_strings\n",
    "\n",
    "        replaced_string = self.textNormalization(string = string)\n",
    "        return dict_found_strings, replaced_string\n",
    "    \n",
    "    def cleaning(self, data):\n",
    "        '''\n",
    "        Input: Data\n",
    "        Process: This method call the methods that we consider necessary for the preprocessing of the tweets database.\n",
    "        In this case we select the text normalization, the stemmatization words, and the remove noise methods. This \n",
    "        method receives in this case a column of the dataframe and returns the column of the dataframe preprocesed.\n",
    "        Return: Preprocessed Data\n",
    "        '''\n",
    "        #text Normalization\n",
    "        data = data.apply(self.textNormalization)\n",
    "        \n",
    "        #stem words\n",
    "        data = data.apply(self.stemWords)\n",
    "        \n",
    "        #Remove Noise\n",
    "        data = data.apply(self.removeNoise)\n",
    "        \n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweets.csv\")\n",
    "tweets = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = PreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaning_tweets'] = prep.cleaning(tweets)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['tweet'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = df.to_numpy().tolist()\n",
    "c = df['senti'].to_numpy().tolist()\n",
    "C = list(set(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(D, C):\n",
    "    # initialize logprior, loglikelihood\n",
    "    logprior = {}\n",
    "    loglikelihood = {}\n",
    "    V = set()\n",
    "    \n",
    "    # for each class c in C\n",
    "    for c in C:\n",
    "        N_doc = len(D)\n",
    "        #number of documents from D in class C\n",
    "        N_c = 0\n",
    "\n",
    "        # for each document d in D\n",
    "        for d in D:\n",
    "            # if document d is in class c\n",
    "            if d[0] == c:\n",
    "                # increment N_c\n",
    "                N_c += 1\n",
    "                # for each word w in d\n",
    "                for w in d[1]:\n",
    "                    #w = w.split(' ')\n",
    "                    # add word w to V\n",
    "                    V.add(w)\n",
    "                    # if word w is not in loglikelihood[c]\n",
    "                    #if w not in loglikelihood[c]:\n",
    "                        # add word w to loglikelihood[c]\n",
    "                        #loglikelihood[c][w] = 0\n",
    "                    # increment loglikelihood[c][w]\n",
    "                    #loglikelihood[c][w] += 1\n",
    "        # compute logprior[c]\n",
    "        logprior[c] = np.log(N_c/N_doc)\n",
    "\n",
    "        # for each word w in V\n",
    "        #for w in V:\n",
    "            # compute loglikelihood[c][w]\n",
    "            #loglikelihood[c][w] = np.log((loglikelihood[c][w] + 1)/(N_c + len(V)))\n",
    "    return logprior, V #, loglikelihood, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_naive_bayes(D, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_naive_bayes(testdoc, logprior, loglikelihood, C,V):\n",
    "    #output array containing the probability for each class\n",
    "    suma = np.zeros(len(C))\n",
    "    # for each class c in C\n",
    "    for c in C:\n",
    "        # compute logprior[c]\n",
    "        suma[c] = logprior[c]\n",
    "        # for each word w in testdoc\n",
    "        for w in testdoc:\n",
    "            # if w is in V\n",
    "            if w in V:\n",
    "                # compute loglikelihood[c][w]\n",
    "                suma[c] += loglikelihood[c][w]\n",
    "                \n",
    "        return suma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
